{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57405d6-5a6c-4ee3-ad72-983ec7c067ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "import tiktoken\n",
    "import timeout_decorator\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pickle\n",
    "import nltk\n",
    "import ast\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from apify_client import ApifyClient\n",
    "from nltk.corpus import stopwords\n",
    "from collections import deque\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keybert import KeyBERT\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4070cad2-e820-48b3-a2e5-b3bf9f4164f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_KEY\")\n",
    "#openai.api_type = \"azure\"\n",
    "#openai.api_version = \"2023-07-01-preview\"\n",
    "#openai.api_base = os.getenv(\"AZURE_BASE\")\n",
    "#openai.api_key = os.getenv(\"AZURE_KEY\")\n",
    "client = OpenAI()\n",
    "optimal_k = 3 #NUMBER OF CLUSTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5be069-67f7-401c-90b4-fbd4731a25d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def search(df, text, n=3, pprint=True):\n",
    "    embedding = np.array(get_embedding(text)).reshape(1, -1)\n",
    "    df['similarity'] = df.embedding.apply(lambda x: cosine_similarity(np.array(x).reshape(1, -1), embedding))\n",
    "    res = df.sort_values('similarity', ascending=False).head(n)\n",
    "    return res\n",
    "\n",
    "def sim(text, target):\n",
    "    embedding = get_embedding(text)\n",
    "    return cosine_similarity(embedding, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3db37-5301-436c-95ca-d3737e57f870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('federalai_embed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0649d2-ee98-40ee-ac36-ebe241567361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['embedding'] = df['embedding'].apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b697a-5d62-4e62-9347-00214c6d6eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['embedding'] = df.Summary.apply(lambda x: get_embedding(x)) #Replace text with column name of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514beb0-2714-429f-bdd9-b505c7b81db3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"federalai_embed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4628d91-f49b-4532-a174-f3c15313796c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Office']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0d0123-919a-40f6-be7e-899fc90b4f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Techniques'] = df['Techniques'].str.replace('Artificial Intelligence Unknown', 'Artificial Intelligence', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a34bc8-6203-4942-8c73-81e306b2a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df['Techniques'] is already defined\n",
    "techniques_series = df['Techniques'].str.split(', ')\n",
    "exploded_techniques = techniques_series.explode()\n",
    "\n",
    "# Count occurrences and remove 'Other' before selecting the top 5\n",
    "technique_counts = exploded_techniques.value_counts().reset_index()\n",
    "technique_counts.columns = ['Technique', 'Count']\n",
    "\n",
    "# Filter out 'Other', if present\n",
    "#technique_counts = technique_counts[technique_counts['Technique'] != 'Other']\n",
    "\n",
    "# Select the top 5 techniques\n",
    "top_technique_counts = technique_counts.head(5)\n",
    "\n",
    "# Create the Plotly bar graph for the top 5 techniques\n",
    "bar_fig = px.bar(top_technique_counts, x='Technique', y='Count', title='Frequency of Top 5 Techniques')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b0a82-bf1e-47ca-9220-8018733d1e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "all_techniques = ' '.join(exploded_techniques.fillna(''))\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width = 800, height = 400, background_color ='white').generate(all_techniques)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Don't show axes for a cleaner look\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a06e2cc-c2cb-4111-9eee-3811d6e658fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cb1178e-6e5d-4c8b-9d20-788420cb1f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "embeddings_matrix = np.vstack(df['embedding'])\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embeddings_matrix)\n",
    "embeddings = df['embedding'].tolist()\n",
    "if isinstance(embeddings[0], str):\n",
    "    embeddings = [ast.literal_eval(e) for e in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fcfb232-b395-44da-82ef-3fe7e261c452",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/skacholia/opt/anaconda3/envs/rta/lib/python3.9/site-packages/threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "embeddings_matrix = np.vstack(df['embedding'])\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embeddings_matrix)\n",
    "embeddings = df['embedding'].tolist()\n",
    "if isinstance(embeddings[0], str):\n",
    "    embeddings = [ast.literal_eval(e) for e in embeddings]\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "df['cluster'] = kmeans.fit_predict(scaled_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b6c2a7-b184-4c37-8cbe-ce8b7bc68bce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine the optimal number of clusters using the elbow method\n",
    "inertia = []\n",
    "K = range(1, 11)  # Assuming we want to test 1 through 10 clusters\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(scaled_embeddings)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the elbow plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K, inertia, 'bo-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac555b-c27e-4223-847e-a04a397e4e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsne_embeddings = TSNE(n_components=3, random_state=42).fit_transform(np.array(embeddings))\n",
    "x = tsne_embeddings[:, 0]\n",
    "y = tsne_embeddings[:, 1]\n",
    "z = tsne_embeddings[:, 2]\n",
    "\n",
    "scatter = go.Scatter3d(\n",
    "    x = x,\n",
    "    y = y,\n",
    "    z = z,\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 5,  # Increase the marker size for better visibility\n",
    "        color = df['cluster'],  # Color by cluster\n",
    "        colorscale = 'Viridis',\n",
    "        opacity = 0.8,\n",
    "    ),\n",
    "    hovertemplate = \n",
    "        '%{text}<br><br>' +  # Bold name on hover\n",
    "        '<b>Cluster: %{customdata}<br>' +  # Include cluster information\n",
    "        'Coordinates: (%{x}, %{y}, %{z})<extra></extra>',  # Include coordinates\n",
    "    text = ['<br>'.join(text[i:i+30] for i in range(0, len(text), 20)) for text in df['Summary']],\n",
    "    customdata = df['cluster']\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = '3D t-SNE Clustering',\n",
    "    scene = dict(\n",
    "        xaxis = dict(title='Dimension 1', zeroline=False),\n",
    "        yaxis = dict(title='Dimension 2', zeroline=False),\n",
    "        zaxis = dict(title='Dimension 3', zeroline=False),\n",
    "    ),\n",
    "    hoverlabel = dict(\n",
    "        bgcolor = \"white\",  # Background color for hover label\n",
    "        font_size = 12,  # Text font size\n",
    "        font_family = \"Arial\"  # Text font family\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[scatter], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b64c1-3b06-4af4-a750-042cd445d8e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kw_model = KeyBERT()\n",
    "stop_words = stopwords.words('english')\n",
    "df['keywords'] = df['Summary'].apply(lambda x: kw_model.extract_keywords(x, keyphrase_ngram_range=(1, 1), stop_words=stop_words))\n",
    "#clusters = kmeans.labels_\n",
    "\n",
    "num_examples = 15 # Adjust this value based on your needs\n",
    "\n",
    "for cluster_idx in range(8):\n",
    "    cluster_data = np.array(scaled_embeddings)[clusters == cluster_idx]\n",
    "    distances = [np.linalg.norm(embedding - kmeans.cluster_centers_[cluster_idx]) for embedding in cluster_data]\n",
    "    closest_examples_idx = np.argsort(distances)[:num_examples]\n",
    "\n",
    "    print(f\"\\nCluster {cluster_idx}:\")\n",
    "    for idx in closest_examples_idx:\n",
    "        print(f\"- {df.loc[idx, 'Summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd598c-5aea-4646-a53c-33acd298931d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_keywords = set()\n",
    "for keywords in df['keywords']:\n",
    "    all_keywords.update(kw for kw, _ in keywords if isinstance(kw, tuple))\n",
    "\n",
    "cluster_keyphrases_unigrams = {}\n",
    "for cluster in df['cluster'].unique():\n",
    "    cluster_data = df[df['cluster'] == cluster]\n",
    "    print(cluster_data)\n",
    "    keyphrase_scores_unigrams = {}\n",
    "    for keyword in all_keywords:\n",
    "        scores_unigrams = []\n",
    "        for kws in cluster_data['keywords']:\n",
    "            for item in kws:\n",
    "                kw, score = item\n",
    "                scores_unigrams.append(score)\n",
    "        scores_unigrams = [score for kws in cluster_data['keywords'] if kws and kws[0][0] == keyword for _, score in kws]\n",
    "        if scores_unigrams:\n",
    "            median_value = np.median(scores_unigrams)\n",
    "            keyphrase_scores_unigrams[keyword] = (median_value * len(scores_unigrams)).round(3)\n",
    "    cluster_keyphrases_unigrams[cluster] = sorted(keyphrase_scores_unigrams.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "print(cluster_keyphrases_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f961806d-7f4e-4a6c-9fb0-240c7ee19fab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Keywords1'] = df['Summary'].apply(lambda x: kw_model.extract_keywords(x, keyphrase_ngram_range=(1, 1), stop_words=stop_words))\n",
    "df['Keywords2'] = df['Summary'].apply(lambda x: kw_model.extract_keywords(x, keyphrase_ngram_range=(2, 2), stop_words=stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c39ff1-506e-44d2-9dda-565f609c77ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/skacholia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/skacholia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/skacholia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download the required NLTK data if not already present\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize the stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Extract all unique keywords and perform stemming or lemmatization\n",
    "all_keywords = set()\n",
    "for keywords in df['Keywords1']:\n",
    "    for keyword, _ in keywords:\n",
    "        keyword = lemmatizer.lemmatize(keyword)\n",
    "        all_keywords.add(keyword)\n",
    "\n",
    "# Extract all unique keywords and perform stemming or lemmatization\n",
    "for keywords in df['Keywords2']:\n",
    "    for keyword, _ in keywords:\n",
    "        keyword = lemmatizer.lemmatize(keyword)\n",
    "        all_keywords.add(keyword)\n",
    "\n",
    "cluster_keyphrases_bigrams = {}\n",
    "cluster_keyphrases_unigrams = {}\n",
    "\n",
    "for cluster in df['cluster'].unique():\n",
    "    cluster_data = df[df['cluster'] == cluster]\n",
    "\n",
    "    keyphrase_scores_bigrams = {}\n",
    "    keyphrase_scores_unigrams = {}\n",
    "\n",
    "    for keyword in all_keywords:\n",
    "        \n",
    "        # Scores for bigrams\n",
    "        scores_bigrams = [score for kws in cluster_data['Keywords2'] for kw, score in kws if lemmatizer.lemmatize(kw) == keyword]\n",
    "        if scores_bigrams:\n",
    "            median_value = np.median(scores_bigrams)\n",
    "            count = len(scores_bigrams)\n",
    "            keyphrase_scores_bigrams[keyword] = (median_value * count).round(3)\n",
    "        \n",
    "        # Scores for unigrams\n",
    "        scores_unigrams = [score for kws in cluster_data['Keywords1'] for kw, score in kws if lemmatizer.lemmatize(kw) == keyword]\n",
    "        if scores_unigrams:\n",
    "            median_value = np.median(scores_unigrams)\n",
    "            count = len(scores_unigrams)\n",
    "            keyphrase_scores_unigrams[keyword] = (median_value * count).round(3)\n",
    "\n",
    "    sorted_keyphrases_bigrams = sorted(keyphrase_scores_bigrams.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "    sorted_keyphrases_unigrams = sorted(keyphrase_scores_unigrams.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "\n",
    "    cluster_keyphrases_bigrams[cluster] = sorted_keyphrases_bigrams\n",
    "    cluster_keyphrases_unigrams[cluster] = sorted_keyphrases_unigrams\n",
    "\n",
    "cluster_df_bigrams = pd.DataFrame.from_dict(cluster_keyphrases_bigrams)\n",
    "cluster_df_unigrams = pd.DataFrame.from_dict(cluster_keyphrases_unigrams)\n",
    "\n",
    "# Change names of columns based on top unigrams\n",
    "new_columns = ['_'.join([keyword for keyword, _ in cluster_df_unigrams[col].head(3)]) for col in cluster_df_bigrams.columns]\n",
    "cluster_df_bigrams.columns = new_columns\n",
    "cluster_df_unigrams.columns = new_columns\n",
    "\n",
    "print(cluster_df_bigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
