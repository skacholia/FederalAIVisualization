{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57405d6-5a6c-4ee3-ad72-983ec7c067ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "import tiktoken\n",
    "import timeout_decorator\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pickle\n",
    "import nltk\n",
    "import ast\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from apify_client import ApifyClient\n",
    "from nltk.corpus import stopwords\n",
    "from collections import deque\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keybert import KeyBERT\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4070cad2-e820-48b3-a2e5-b3bf9f4164f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_KEY\")\n",
    "#openai.api_type = \"azure\"\n",
    "#openai.api_version = \"2023-07-01-preview\"\n",
    "#openai.api_base = os.getenv(\"AZURE_BASE\")\n",
    "#openai.api_key = os.getenv(\"AZURE_KEY\")\n",
    "client = OpenAI()\n",
    "optimal_k = 3 #NUMBER OF CLUSTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5be069-67f7-401c-90b4-fbd4731a25d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def search(df, text, n=3, pprint=True):\n",
    "    embedding = np.array(get_embedding(text)).reshape(1, -1)\n",
    "    df['similarity'] = df.embedding.apply(lambda x: cosine_similarity(np.array(x).reshape(1, -1), embedding))\n",
    "    res = df.sort_values('similarity', ascending=False).head(n)\n",
    "    return res\n",
    "\n",
    "def sim(text, target):\n",
    "    embedding = get_embedding(text)\n",
    "    return cosine_similarity(embedding, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3db37-5301-436c-95ca-d3737e57f870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('federalai_embed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0649d2-ee98-40ee-ac36-ebe241567361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['embedding'] = df['embedding'].apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b697a-5d62-4e62-9347-00214c6d6eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['embedding'] = df.Summary.apply(lambda x: get_embedding(x)) #Replace text with column name of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514beb0-2714-429f-bdd9-b505c7b81db3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"federalai_embed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4628d91-f49b-4532-a174-f3c15313796c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Office']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0d0123-919a-40f6-be7e-899fc90b4f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Techniques'] = df['Techniques'].str.replace('Artificial Intelligence Unknown', 'Artificial Intelligence', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a34bc8-6203-4942-8c73-81e306b2a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df['Techniques'] is already defined\n",
    "techniques_series = df['Techniques'].str.split(', ')\n",
    "exploded_techniques = techniques_series.explode()\n",
    "\n",
    "# Count occurrences and remove 'Other' before selecting the top 5\n",
    "technique_counts = exploded_techniques.value_counts().reset_index()\n",
    "technique_counts.columns = ['Technique', 'Count']\n",
    "\n",
    "# Filter out 'Other', if present\n",
    "#technique_counts = technique_counts[technique_counts['Technique'] != 'Other']\n",
    "\n",
    "# Select the top 5 techniques\n",
    "top_technique_counts = technique_counts.head(5)\n",
    "\n",
    "# Create the Plotly bar graph for the top 5 techniques\n",
    "bar_fig = px.bar(top_technique_counts, x='Technique', y='Count', title='Frequency of Top 5 Techniques')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b0a82-bf1e-47ca-9220-8018733d1e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "all_techniques = ' '.join(exploded_techniques.fillna(''))\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width = 800, height = 400, background_color ='white').generate(all_techniques)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Don't show axes for a cleaner look\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a06e2cc-c2cb-4111-9eee-3811d6e658fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cb1178e-6e5d-4c8b-9d20-788420cb1f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "embeddings_matrix = np.vstack(df['embedding'])\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embeddings_matrix)\n",
    "embeddings = df['embedding'].tolist()\n",
    "if isinstance(embeddings[0], str):\n",
    "    embeddings = [ast.literal_eval(e) for e in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fcfb232-b395-44da-82ef-3fe7e261c452",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/skacholia/opt/anaconda3/envs/rta/lib/python3.9/site-packages/threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "embeddings_matrix = np.vstack(df['embedding'])\n",
    "scaler = StandardScaler()\n",
    "scaled_embeddings = scaler.fit_transform(embeddings_matrix)\n",
    "embeddings = df['embedding'].tolist()\n",
    "if isinstance(embeddings[0], str):\n",
    "    embeddings = [ast.literal_eval(e) for e in embeddings]\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "df['cluster'] = kmeans.fit_predict(scaled_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b6c2a7-b184-4c37-8cbe-ce8b7bc68bce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine the optimal number of clusters using the elbow method\n",
    "inertia = []\n",
    "K = range(1, 11)  # Assuming we want to test 1 through 10 clusters\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(scaled_embeddings)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the elbow plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K, inertia, 'bo-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac555b-c27e-4223-847e-a04a397e4e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsne_embeddings = TSNE(n_components=3, random_state=42).fit_transform(np.array(embeddings))\n",
    "x = tsne_embeddings[:, 0]\n",
    "y = tsne_embeddings[:, 1]\n",
    "z = tsne_embeddings[:, 2]\n",
    "\n",
    "scatter = go.Scatter3d(\n",
    "    x = x,\n",
    "    y = y,\n",
    "    z = z,\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 5,  # Increase the marker size for better visibility\n",
    "        color = df['cluster'],  # Color by cluster\n",
    "        colorscale = 'Viridis',\n",
    "        opacity = 0.8,\n",
    "    ),\n",
    "    hovertemplate = \n",
    "        '%{text}<br><br>' +  # Bold name on hover\n",
    "        '<b>Cluster: %{customdata}<br>' +  # Include cluster information\n",
    "        'Coordinates: (%{x}, %{y}, %{z})<extra></extra>',  # Include coordinates\n",
    "    text = ['<br>'.join(text[i:i+30] for i in range(0, len(text), 20)) for text in df['Summary']],\n",
    "    customdata = df['cluster']\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = '3D t-SNE Clustering',\n",
    "    scene = dict(\n",
    "        xaxis = dict(title='Dimension 1', zeroline=False),\n",
    "        yaxis = dict(title='Dimension 2', zeroline=False),\n",
    "        zaxis = dict(title='Dimension 3', zeroline=False),\n",
    "    ),\n",
    "    hoverlabel = dict(\n",
    "        bgcolor = \"white\",  # Background color for hover label\n",
    "        font_size = 12,  # Text font size\n",
    "        font_family = \"Arial\"  # Text font family\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[scatter], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb7b64c1-3b06-4af4-a750-042cd445d8e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0:\n",
      "Department: Fermi National Accelerator\n",
      "Title: Simulation-based inference for cosmology\n",
      "Summary: This project will develop and use simulation-based inference to estimate \n",
      "cosmological parameters related to cosmic acceleration in the early and \n",
      "late universe — via the cosmic microwave background and strong \n",
      "gravitational lensing, respectively. This will produce an analysis pipeline \n",
      "that can be deployed for next-generation cosmic surveys.\n",
      "\n",
      "\n",
      "Department: Customs and Border Protection\n",
      "Title: Geospatial imagery utilizing annotation\n",
      "Summary: Leverages a commercial constellation of Synthetic Aperture Radar (SAR) satellites with readily available data, capable of imaging any location on Earth, day, and night, regardless of cloud cover. \n",
      "\n",
      "Utilizes AI, including machine vision, object, detection, object recognition, and annotation to detect airframes, military vehicles, and marine vessels, as well as built-in change detection capabilities for disaster response missions.\n",
      "\n",
      "\n",
      "Department: International Trade Administration (ITA)\n",
      "Title: Consolidated Screening List\n",
      "Summary: The Consolidated Screening List (CSL) is a list of parties for which the United States \n",
      "Government maintains restrictions on certain exports, reexports, or transfers of items. It \n",
      "consists of the consolidation of 13 export screening lists of the Departments of \n",
      "Commerce, State, and Treasury.  The CSL search engine has “Fuzzy Name Search” \n",
      "capabilities, allowing a search without knowing the exact spelling of an entity’s name. In \n",
      "Fuzzy Name mode, the CSL returns a “score” for results that exactly or nearly match the \n",
      "searched name. This is particularly helpful when searching on CSL for names that have \n",
      "been translated into English from non-Latin alphabet languages.\n",
      "\n",
      "\n",
      "Department: Customs and Border Protection\n",
      "Title: Data and Entity Resolution\n",
      "Summary: Automates data unification and entity resolution with a high level of trust at enterprise scale and speed.\n",
      "\n",
      "Data and Entity Resolution uses Machine Learning modeling to ingest multiple data sources and develop models that associate disparate records to identify probable connections, unique entities, and/or identify commonalities between multiple independently submitted records.\n",
      "\n",
      "The automation of entity resolution within the models is supported by a tool that enables non-technical end users to continuously train models through a user-friendly interface. \n",
      "\n",
      "\n",
      "Department: National Oceanic and Atmospheric Administration (NOAA)\n",
      "Title: AI-based automation of acoustic detection of marine mammals\n",
      "Summary: Timely processing of these data is critical for adapting mitigation measures as climate \n",
      "change continues to impact Arctic marine mammals.  Infrastructure for Noise and \n",
      "Soundscape Tolerant Investigation of Nonspecific Call Types (INSTINCT) is command line \n",
      "software which was developed in-house for model training, evaluation, and deployment \n",
      "of machine learning models for the purpose of marine mammal detection in passive \n",
      "acoustic data. It also includes annotation workflows for labeling and validation. INSTINCT \n",
      "has been successfully deployed in several analyses, and further development of detectors \n",
      "within INSTINCT is desired for future novel studies and automation. Continued integration \n",
      "of AI methods into existing processes of the CAEP acoustics group requires a skilled \n",
      "operator familiar with INSTINCT, machine learning, and acoustic repertoire of Alaska \n",
      "region marine mammals.\n",
      "\n",
      "\n",
      "Department: International Trade Administration (ITA)\n",
      "Title: B2B Matchmaking\n",
      "Summary: The system's algorithms and AI technology qualifies data and makes B2B matches with \n",
      "event participants according to their specific needs and available opportunities.  The \n",
      "systems inputs are data related to event participants and the outputs are suggested B2B \n",
      "matches between participants and a match strength scorecard.\n",
      "\n",
      "\n",
      "Department: Customs and Border Protection\n",
      "Title: AI for Autonomous Situational Awareness\n",
      "Summary: The AI for autonomous situational awareness system is intended to use IoT sensor kits to covertly detect and track illicit cross-border traffic in remote locations. \n",
      "\n",
      "The system will leverage a motion image/video system enhanced with Artificial Intelligence that is capable of vehicle detection and direction determination. It will also incorporate a motion sensor that, when triggered, wakes up a high-resolution camera to capture a series of pictures, with additional sensors providing confirmation prior to camera capture. \n",
      "\n",
      "Images captured will be processed by Artificial Intelligence models to classify objects, determine vehicle direction at intersections, and provide imagery sufficient for re-identification. Ultimately, the systems is intended to create a low footprint, low cost, low power system to provide situational awareness and covert detection.\n",
      "\n",
      "\n",
      "Department: Customs and Border Protection\n",
      "Title: Automated Item of Interest Detection - ICAD\n",
      "Summary: The software analyzes photographs that are taken by field imaging equipment, which are then fed into the ICAD system for review by USBP agents and personnel. The Matroid software currently processes and annotates images using proprietary software to determine if any of the images contain human subjects.\n",
      "\n",
      "Matroid is the name of the Video Computer Aided Detection system used by CBP. It uses trained computer vision models that recognize objects, people, and events in any image or video stream. Once a detector is trained, it can monitor streaming video in real time, or efficiently search through pre-recorded video data or images to identify objects, people, and events of interest. \n",
      "\n",
      "The intent for the ICAD system is to expand the models used to vehicles, and subjects with long-arm rifles, while excluding items of little or no interest such as animals.\n",
      "\n",
      "\n",
      "Department: Fermi National Accelerator\n",
      "Title: Next-Generation Beam Cooling and Control with Optical Stochastic Cooling\n",
      "Summary: This program leverages the physics and technology of optical stochastic \n",
      "cooling (OSC) to explore new possibilities in beam control and sensing.  \n",
      "The planned architecture and performance of a new OSC system at \n",
      "IOTA should enable turn-by-turn programmability of the high-gain OSC.  \n",
      "This capability can then be used in conjunction with other hardware \n",
      "systems as the basis of an action space for reinforcement learning (RL) \n",
      "methods.  The program aims to establish a new state of the art in beam \n",
      "cooling and a flexible set of tools for beam control and sensing at \n",
      "colliders and other accelerator facilities.\n",
      "\n",
      "\n",
      "Department: Cybersecurity and Infrastructure Security Agency\n",
      "Title: Critical Infrastructure Anomaly Alerting\n",
      "Summary: The Cyber Sentry program provides monitoring of critical infrastructure networks. Within the program, threat hunting analysts require advanced anomaly detection and machine learning capabilities to examine multimodal cyber-physical data on IT and OT networks, including ICS/SCADA. The Critical Infrastructure Anomaly Alerting model provides AI-assistance in processing this information.\n",
      "\n",
      "\n",
      "\n",
      "Cluster 1:\n",
      "Department: United States Patent and Trade Office (USPTO)\n",
      "Title: AI retrieval for TM design coding and Image search\n",
      "Summary: Clarivate COTS solution to assist examiner identification of similar trademark images, to \n",
      "suggest the correct assignment of mark image design codes, and to determine the \n",
      "potential acceptability of the identifications of goods and services.  System is anticipated \n",
      "to use both incoming trademark images and registered trademark images and output \n",
      "design codes and/or other related images.\n",
      "\n",
      "\n",
      "Department: United States Citizenship and Immigration Services\n",
      "Title: Predicted to Naturalize\n",
      "Summary: The Predicted to Naturalize model predicts when Legal Permanent Residents would be eligible to naturalize, and attempts to provide a current address. This model could potentially be used to send correspondence to USCIS customers of their resident status, and notify others of potential USCIS benefits.\n",
      "\n",
      "\n",
      "Department: United States Citizenship and Immigration Services\n",
      "Title: Person-Centric Identity Services A-Number Management Model\n",
      "Summary: The vision of Person-Centric Identity Services (PCIS) is to be the authoritative source of trusted biographical and biometric information that provides real-time, two-way visibility between services into an individual's comprehensive immigration history and status. The A-Number Management model ingests person-centric datasets from various source systems for model training and evaluation purposes. The dataset includes biographic information (name, date of birth, Alien #, Social Security #, passport #, etc.) as well as biographic information (fingerprint IDs, eye color, hair color, height, weight, etc.) for model training and matching purposes. \n",
      "\n",
      "The A-Number Management identifies which records from within our identity database best match search criteria. The model uses machine learning to ensure that search results presented to authorized external partners for external integrations and servicing have a high degree of confidence with the search criteria so that trust in the PCIS entity resolution remains high.\n",
      "\n",
      "The A-Number Management model plays a critical role in the entity resolution and surfacing of a person and all their associated records. The machine learning models are more capable of resolving \"fuzzy\" matches, and deal with the reality of different data quality.\n",
      "\n",
      "\n",
      "Department: Immigration and Customs Enforcement\n",
      "Title: Mobile Device Analytics\n",
      "Summary: Mobile Device Analytics (MDA) has been developed to meet the demand on investigators to view and analyze massive amounts of data resulting from court ordered mobile device extractions.  The overarching goal of MDA is to improve the efficacy of agents and analysts in identifying pertinent evidence, relationships, and criminal networks from data extracted from cellular phones. Machine Learning is being developed for object detection (such as firearms, drugs, money, etc.) in photos and videos contained in the data.\n",
      "\n",
      "This is a DHS HSI Innovation Lab / RAVEn project. The Repository for Analytics in a Virtualized Environment (RAVEn) facilitates large, complex analytical projects to support ICE’s mission to enforce and investigate violations of U.S. criminal, civil, and administrative laws. RAVEn also enables tools used to analyze trends and isolate criminal patterns as HSI mission needs arise. For more information, please read the DHS/ICE/PIA-055 - Privacy Impact Assessment 055 for RAVEn.\n",
      "\n",
      "\n",
      "Department: United States Citizenship and Immigration Services\n",
      "Title: Topic Modeling on Request For Evidence data sets\n",
      "Summary: Builds models that identify lists of topics and documents that are related to each topic. Topic Modeling provides methods for automatically organizing, understanding, searching, and summarizing text data. It can help with the following: discovering the hidden themes in the collection. classifying the documents into the discovered themes.\n",
      "\n",
      "\n",
      "Department: United States Citizenship and Immigration Services\n",
      "Title: Sentiment Analysis - Surveys\n",
      "Summary: The Sentiment Analysis - Surveys system provides a statistical analysis of quantitative results from survey results and then uses Natural Language Processing (NLP) modeling software to assign \"sentiments\" to categories ranging from strongly positive to strongly negative. This allows survey administrators to glean valuable information from employee satisfaction surveys from both quantitative and qualitative data. This capability is currently available on demand.\n",
      "\n",
      "\n",
      "Department: National Oceanic and Atmospheric Administration (NOAA)\n",
      "Title: Passive acoustic analysis using ML in Cook Inlet, AK\n",
      "Summary: Passive acoustic data is analyzed for detection of beluga whales and classification of the \n",
      "different signals emitted by these species. Detection and classification are done with an \n",
      "ensemble of 4 CNN models and weighted scoring developed in collaboration with \n",
      "Microsoft. Results are being used to inform seasonal distribution, habitat use, and impact \n",
      "from anthropogenic disturbance within Cook Inlet beluga critical habitat. The project is \n",
      "aimed to expand to other cetacean species as well as anthropogenic noise.\n",
      "\n",
      "\n",
      "Department: Customs and Border Protection\n",
      "Title: Automated Item of Interest Detection - ICAD\n",
      "Summary: The software analyzes photographs that are taken by field imaging equipment, which are then fed into the ICAD system for review by USBP agents and personnel. The Matroid software currently processes and annotates images using proprietary software to determine if any of the images contain human subjects.\n",
      "\n",
      "Matroid is the name of the Video Computer Aided Detection system used by CBP. It uses trained computer vision models that recognize objects, people, and events in any image or video stream. Once a detector is trained, it can monitor streaming video in real time, or efficiently search through pre-recorded video data or images to identify objects, people, and events of interest. \n",
      "\n",
      "The intent for the ICAD system is to expand the models used to vehicles, and subjects with long-arm rifles, while excluding items of little or no interest such as animals.\n",
      "\n",
      "\n",
      "Department: National Oceanic and Atmospheric Administration (NOAA)\n",
      "Title: Fisheries Electronic Monitoring Image Library\n",
      "Summary: The Fisheries Electronic Monitoring Library (FEML) will be the central repository for \n",
      "electronic monitoring (EM) data related to marine life.\n",
      "\n",
      "\n",
      "Department: Customs and Border Protection\n",
      "Title: Autonomous Maritime Awareness\n",
      "Summary: The Autonomous Maritime Awareness system combines surveillance towers, ocean data solutions, unmanned autonomous surface vehicles (ASV), and AI to autonomously detect, identify, and track items of interest in a maritime environment.\n",
      "\n",
      "The towers are low-cost, customizable, and relocatable surveillance systems. They are equipped with a suite of radars and day/night camera sensors. The ASVs have been ruggedized for the open ocean and are powered by wind, solar, and/or onboard engine as required, allowing them to operate in an area of responsibility (AOR) for up to 12 months. Their sensor suite includes cameras and radar. \n",
      "\n",
      "Both systems use AI/ML to detect and identify objects, determine items of interest (IoI) and autonomously track those items using their sensor suites. Once identified, these systems can send alerts to monitoring agencies for at-sea interdictions of potential targets and/or intel collections.\n",
      "\n",
      "\n",
      "\n",
      "Cluster 2:\n",
      "Department: International Trade Administration (ITA)\n",
      "Title: Market Diversification Toolkit\n",
      "Summary: The Market Diversification Tool identifies potential new export markets using current \n",
      "trade patterns. A user enters what products they make and the markets they currently \n",
      "export to.  The Market Diversification Tool applies a ML algorithm to identify and compare \n",
      "markets that should be considered.  The tool brings together product-specific trade and \n",
      "tariff data and economy-level macroeconomic and governance data to provide a picture \n",
      "of which markets make sense for further market research. Users can limit the markets in \n",
      "the results to only the ones they want to consider and modify how each of the eleven \n",
      "indicators in the tool contributes to a country’s overall score. Users can export all the data \n",
      "to a spreadsheet for further analysis.\n",
      "\n",
      "\n",
      "Department: Minority Business Development Administration (MBDA)\n",
      "Title: Azure Chatbot\n",
      "Summary: Azure Chatbot is being leveraged to automate and streamline the user response to \n",
      "potential questions for MBDA users while interacting with the external facing MBDA \n",
      "website. The solution leverages AI based chatbot response coupled with Machine \n",
      "Learning and Natural Language Processing capabilities.\n",
      "\n",
      "\n",
      "Department: National Oceanic and Atmospheric Administration (NOAA)\n",
      "Title: Coastal Change Analysis Program (C-CAP)\n",
      "Summary: Beginning in 2015, C-CAP embarked on operational high resolution land cover \n",
      "development effort that utilized geographic object-based image analysis and ML \n",
      "algorithms such as Random Forest to classify coastal land cover from 1m multispectral \n",
      "imagery. More recently, C-CAP has been relying on a CNN approach for the deriving the \n",
      "impervious surface component of their land cover products. The majority of the work is \n",
      "accomplished through external contracts. Prior to the high-res effort, C-CAP focused on \n",
      "developing Landsat based moderate resolution multi-date land cover for the coastal U.S. \n",
      "In 2002, C-CAP adopted a methodology that employed Classification and Regression Trees \n",
      "for land cover data development.\n",
      "\n",
      "\n",
      "Department: Fermi National Accelerator\n",
      "Title: In-storage computing for multi- messenger astronomy in neutrino experiments and cosmological surveys\n",
      "Summary: This project aims to address the big-data challenges and stringent time \n",
      "constraints facing multi-messenger astronomy (MMA) in neutrino \n",
      "experiments and cosomological surveys. Instead of following the \n",
      "traditional computing paradigm of moving data to the compute \n",
      "elements, it does the opposite to embed computation in the data where \n",
      "processing is performed in situ. This will be achieved through emerging \n",
      "computational storage accelerators on which ML algorithms may be \n",
      "deployed to execute MMA tasks quickly so alerts can be disseminated \n",
      "promptly.\n",
      "\n",
      "\n",
      "Department: United States Citizenship and Immigration Services\n",
      "Title: Person-Centric Identity Services A-Number Management Model\n",
      "Summary: The vision of Person-Centric Identity Services (PCIS) is to be the authoritative source of trusted biographical and biometric information that provides real-time, two-way visibility between services into an individual's comprehensive immigration history and status. The A-Number Management model ingests person-centric datasets from various source systems for model training and evaluation purposes. The dataset includes biographic information (name, date of birth, Alien #, Social Security #, passport #, etc.) as well as biographic information (fingerprint IDs, eye color, hair color, height, weight, etc.) for model training and matching purposes. \n",
      "\n",
      "The A-Number Management identifies which records from within our identity database best match search criteria. The model uses machine learning to ensure that search results presented to authorized external partners for external integrations and servicing have a high degree of confidence with the search criteria so that trust in the PCIS entity resolution remains high.\n",
      "\n",
      "The A-Number Management model plays a critical role in the entity resolution and surfacing of a person and all their associated records. The machine learning models are more capable of resolving \"fuzzy\" matches, and deal with the reality of different data quality.\n",
      "\n",
      "\n",
      "Department: National Oceanic and Atmospheric Administration (NOAA)\n",
      "Title: FathomNet\n",
      "Summary: FathomNet provides much-needed training data (e.g., annotated, and localized imagery) \n",
      "for developing machine learning algorithms that will enable fast, sophisticated analysis of \n",
      "visual data. We've utilized interns and college class curriculums to localize annotations on \n",
      "NOAA video data for inclusion in FathomNet and to begin training our own algorithms.\n",
      "\n",
      "\n",
      "Department: National Energy Technology Laboratory\n",
      "Title: ANN Submodels of Reaction Physics\n",
      "Summary: ANN development of flow physics for code acceleration\n",
      "\n",
      "\n",
      "Department: National Oceanic and Atmospheric Administration (NOAA)\n",
      "Title: Robotic microscopes and machine learning algorithms remotely and autonomously track lower trophic levels for improved ecosystem monitoring and assessment\n",
      "Summary: Phytoplankton are the foundation of marine food webs supporting fisheries and coastal \n",
      "communities. They respond rapidly to physical and chemical oceanography, and changes \n",
      "in phytoplankton communities can impact the structure and functioning of food webs. We \n",
      "use a robotic microscope called an Imaging Flow Cytobot (IFCB) to continuously collect \n",
      "images of phytoplankton from seawater. Automated taxonomic identification of imaged \n",
      "phytoplankton uses a supervised machine learning approach (random forest algorithm). \n",
      "We deploy the IFCB on fixed (docks) and roving (aboard survey ships) platforms to \n",
      "autonomously monitor phytoplankton communities in aquaculture areas in Puget Sound \n",
      "and in the California Current System. We map the distribution and abundance of \n",
      "phytoplankton functional groups and their relative food value to support fisheries and \n",
      "aquaculture and describe their changes in relation to ocean and climate variability and \n",
      "change.\n",
      "\n",
      "\n",
      "Department: National Energy Technology Laboratory\n",
      "Title: Data discovery, processing, and generation using machine learning for a range of CCS data and information\n",
      "Summary: The team will focus on supporting ongoing geospatial data collection \n",
      "and publishing efforts leveraging the new EDX++ cloud computer \n",
      "capabilities through ArcGIS Enterprise Portal. The use of Arc Enterprise \n",
      "Portal will support the development of the Carbon Matchmaker tool, as \n",
      "well as support the release of a new version of GeoCube, which will be \n",
      "host to the updated Carbon Storage Open Database and NATCARB \n",
      "completed in EY21. NETL is supporting DOE-FECM in developing and \n",
      "releasing a survey and map for the Carbon Matchmaker, a tool \n",
      "developed to enable stakeholders to self-identify carbon dioxide related \n",
      "activities (production, utilization, storage, direct air capture, and \n",
      "infrastructure/transportation) to identify and connect stakeholders and \n",
      "support national collaborative opportunities. The ArcGIS Enterprise \n",
      "Portal will be leveraged to build out a new version of GeoCube with the \n",
      "migration of hundreds of spatial data layers into the new platform. The \n",
      "migration of data to an Arc Enterprise based GeoCube will enable \n",
      "easier version control for data integration and curation.\n",
      "\n",
      "\n",
      "Department: Fermi National Accelerator\n",
      "Title: Streamining intelligent detectors for sPHENIX/EIC\n",
      "Summary: This project develops real-time algorithms for event filtering with tracking \n",
      "detectors for nuclear physics collider experiments.\n",
      "\n",
      "\n",
      "\n",
      "Cluster 3:\n",
      "Department: National Energy Technology Laboratory\n",
      "Title: Solving Field Equations on the Wafer Scale Engine\n",
      "Summary: The intent is to develop a collocated, finite volume code to allow \n",
      "maximum mesh flexibility and support advanced CFD capabilities found \n",
      "in modern CFD codes like Fluent, OpenFOAM, and MFiX.\n",
      "NETL will take a metered approach to development towards a fully \n",
      "reacting CFD capability on the WSE. EY22 will be filled with API \n",
      "capability expansions needed to support general purpose CFD \n",
      "applications, such as general purpose finite volume formulations, \n",
      "collocated grid capabilities (Rhie & Chow Interpolation), bit stuffing to \n",
      "save memory when dealing with cell types, general purpose boundary \n",
      "conditions, etc. In addition, the code will be benchmarked in a series of \n",
      "tests towards a fully reacting CFD capability that will support problems \n",
      "of interest to FECM.\n",
      "\n",
      "\n",
      "Department: National Energy Technology Laboratory\n",
      "Title: To build the first data analytics and artificial intelligence field laboratory for unconventional resources in the Powder River Basin, focusing on optimization of hydraulic fracture stimulations through the use of multiple diagnostic technologies.\n",
      "Summary: To establish a tight oil Field Laboratory in the Powder River Basin and \n",
      "accelerate the development of three major unconventional oil resources \n",
      "through detailed geologic characterization and improved geologic \n",
      "models leading to significant advances in well completion and fracture \n",
      "stimulation designs specific to these three formations. Utilize multi-\n",
      "variate analysis to understand the interrelationship between completion \n",
      "and stimulation controls on well productivity.\n",
      "\n",
      "\n",
      "Department: National Oceanic and Atmospheric Administration (NOAA)\n",
      "Title: Ice seal detection and species classification in multispectral aerial imagery\n",
      "Summary: Refine and improve detection and classification pipelines with the goal of reducing false \n",
      "positive rates (to < 50%) while maintaining > 90% accuracy and significantly reducing or \n",
      "eliminating the labor intensive, post survey review process.\n",
      "\n",
      "\n",
      "Department: National Energy Technology Laboratory\n",
      "Title: To drive insights on the power system reliability, cost, and operations during the energy transition with and without FECM technologies\n",
      "Summary: Commercially available models will be used to generate predictive \n",
      "scenarios\n",
      "\n",
      "\n",
      "Department: National Oceanic and Atmospheric Administration (NOAA)\n",
      "Title: ANN to improve CFS T and P outlooks\n",
      "Summary: Fan Y., Krasnopolsky, V., van den Dool H., Wu, C. , and Gottschalck J. (2021). Using \n",
      "Artificial Neural Networks to Improve CFS Week 3-4 Precipitation and Temperature \n",
      "Forecasts.\n",
      "\n",
      "\n",
      "Department: Customs and Border Protection\n",
      "Title: Geospatial imagery utilizing annotation\n",
      "Summary: Leverages a commercial constellation of Synthetic Aperture Radar (SAR) satellites with readily available data, capable of imaging any location on Earth, day, and night, regardless of cloud cover. \n",
      "\n",
      "Utilizes AI, including machine vision, object, detection, object recognition, and annotation to detect airframes, military vehicles, and marine vessels, as well as built-in change detection capabilities for disaster response missions.\n",
      "\n",
      "\n",
      "Department: National Energy Technology Laboratory\n",
      "Title: To drive insights on pipeline maintenance and repair strategies to reduce incidents of pipeline leakage; support evaluation of use and reuse strategies\n",
      "Summary: ML will be used to develop a pipeline risk assessment geospatial model \n",
      "and support evaluation of use and reuse opportunities.\n",
      "\n",
      "\n",
      "Department: National Energy Technology Laboratory\n",
      "Title: Reduce computational cost of CFD simulations that screen for more efficient intensified solvent contactor geometries.\n",
      "Summary: Collaborate with Subtask 4.3 Machine Learning Support to reduce the \n",
      "computational complexity of validated CFD calculations using Deeper \n",
      "Fluids (DF), graph neural networks (GNNs), or similar ML approaches. \n",
      "Further development of ongoing process modeling/optimization \n",
      "ultimately informed by the CFD reduced order models (ROM) will also \n",
      "be a focus.\n",
      "\n",
      "\n",
      "Department: Customs and Border Protection\n",
      "Title: Automated Item of Interest Detection - ICAD\n",
      "Summary: The software analyzes photographs that are taken by field imaging equipment, which are then fed into the ICAD system for review by USBP agents and personnel. The Matroid software currently processes and annotates images using proprietary software to determine if any of the images contain human subjects.\n",
      "\n",
      "Matroid is the name of the Video Computer Aided Detection system used by CBP. It uses trained computer vision models that recognize objects, people, and events in any image or video stream. Once a detector is trained, it can monitor streaming video in real time, or efficiently search through pre-recorded video data or images to identify objects, people, and events of interest. \n",
      "\n",
      "The intent for the ICAD system is to expand the models used to vehicles, and subjects with long-arm rifles, while excluding items of little or no interest such as animals.\n",
      "\n",
      "\n",
      "Department: National Oceanic and Atmospheric Administration (NOAA)\n",
      "Title: Fisheries Electronic Monitoring Image Library\n",
      "Summary: The Fisheries Electronic Monitoring Library (FEML) will be the central repository for \n",
      "electronic monitoring (EM) data related to marine life.\n",
      "\n",
      "\n",
      "\n",
      "Cluster 4:\n",
      "Department: National Oceanic and Atmospheric Administration (NOAA)\n",
      "Title: Deep learning algorithms to automate right whale photo id\n",
      "Summary: AI for right whale photo id began with a Kaggle competition and has since expanded to \n",
      "include several algorithms to match right whales from different viewpoints (aerial, lateral) \n",
      "and body part (head, fluke, peduncle). The system is now live and operational on the \n",
      "Flukebook platform for both North Atlantic and southern right whales. We have a paper in \n",
      "review at Mammalian Biology.\n",
      "\n",
      "\n",
      "Department: International Trade Administration (ITA)\n",
      "Title: Chatbot Pilot\n",
      "Summary: Chatbot embedded into trade.gov to assist ITA clients with FAQs, locating information and \n",
      "content, suggesting events and services.  ITA clients would enter input into the chatbot in \n",
      "the form of questions or responses to prompts.  The chatbot would scan ITA content \n",
      "libraries and input from ITA staff and return answers and suggestions based on client \n",
      "persona (exporter, foreign buyer, investor).\n",
      "\n",
      "\n",
      "Department: National Oceanic and Atmospheric Administration (NOAA)\n",
      "Title: Automated detection of hazardous low clouds in support of safe and efficient transportation\n",
      "Summary: This is a maintenance and sustainment project for the operational GOES-R fog/low stratus \n",
      "(FLS) products. The FLS products are derived from the combination of GOES-R satellite \n",
      "imagery and NWP data using machine learning. The FLS products, which are available in \n",
      "AWIPS, are routinely used by the NWS Aviation Weather Center and Weather Forecast \n",
      "Offices.\n",
      "\n",
      "\n",
      "Department: Lawrence Livermore National Laboratory\n",
      "Title: Quantum computing and information systems\n",
      "Summary: Machine learning and quantum computing applied towards optimization, \n",
      "quantum chemistry, material science, and cryptography\n",
      "\n",
      "\n",
      "Department: National Oceanic and Atmospheric Administration (NOAA)\n",
      "Title: BANTER, a machine learning acoustic event classifier\n",
      "Summary: A supervised machine learning acoustic event classifier using hierarchical random forests\n",
      "\n",
      "\n",
      "Department: National Oceanic and Atmospheric Administration (NOAA)\n",
      "Title: ANN to improve CFS T and P outlooks\n",
      "Summary: Fan Y., Krasnopolsky, V., van den Dool H., Wu, C. , and Gottschalck J. (2021). Using \n",
      "Artificial Neural Networks to Improve CFS Week 3-4 Precipitation and Temperature \n",
      "Forecasts.\n",
      "\n",
      "\n",
      "Department: National Energy Technology Laboratory\n",
      "Title: Database will be utilized to demonstrate targeted biocide strategies using AI to assess large DNA datasets.\n",
      "Summary: The team will develop a public DNA database that will advance \n",
      "knowledge in produced water management. This project consists of two \n",
      "phases: (1) the development and launching of the database, and (2) the \n",
      "demonstration of applicability of the database by conducting a network \n",
      "analysis. The work will be pursued as defined in the phases below. The \n",
      "fully characterized streams will be used by other FWPs to estimate \n",
      "overall resource recovery and will be used by other FWPs as training \n",
      "set for machine learning (ML) models to predict compositions when only \n",
      "limited measurements can or have been completed for the produced \n",
      "water.\n",
      "\n",
      "\n",
      "Department: National Oceanic and Atmospheric Administration (NOAA)\n",
      "Title: Coastal Change Analysis Program (C-CAP)\n",
      "Summary: Beginning in 2015, C-CAP embarked on operational high resolution land cover \n",
      "development effort that utilized geographic object-based image analysis and ML \n",
      "algorithms such as Random Forest to classify coastal land cover from 1m multispectral \n",
      "imagery. More recently, C-CAP has been relying on a CNN approach for the deriving the \n",
      "impervious surface component of their land cover products. The majority of the work is \n",
      "accomplished through external contracts. Prior to the high-res effort, C-CAP focused on \n",
      "developing Landsat based moderate resolution multi-date land cover for the coastal U.S. \n",
      "In 2002, C-CAP adopted a methodology that employed Classification and Regression Trees \n",
      "for land cover data development.\n",
      "\n",
      "\n",
      "Department: National Oceanic and Atmospheric Administration (NOAA)\n",
      "Title: Picky\n",
      "Summary: Using CNN to pick out objects of a particular size from sides scan imagery.  Presents users \n",
      "with a probability that allows for automation of contact picking in the field.  Side scan \n",
      "imagery is simple one channel intensity image which lends itself well to basic CNN \n",
      "techniques.\n",
      "\n",
      "\n",
      "Department: National Oceanic and Atmospheric Administration (NOAA)\n",
      "Title: Robotic microscopes and machine learning algorithms remotely and autonomously track lower trophic levels for improved ecosystem monitoring and assessment\n",
      "Summary: Phytoplankton are the foundation of marine food webs supporting fisheries and coastal \n",
      "communities. They respond rapidly to physical and chemical oceanography, and changes \n",
      "in phytoplankton communities can impact the structure and functioning of food webs. We \n",
      "use a robotic microscope called an Imaging Flow Cytobot (IFCB) to continuously collect \n",
      "images of phytoplankton from seawater. Automated taxonomic identification of imaged \n",
      "phytoplankton uses a supervised machine learning approach (random forest algorithm). \n",
      "We deploy the IFCB on fixed (docks) and roving (aboard survey ships) platforms to \n",
      "autonomously monitor phytoplankton communities in aquaculture areas in Puget Sound \n",
      "and in the California Current System. We map the distribution and abundance of \n",
      "phytoplankton functional groups and their relative food value to support fisheries and \n",
      "aquaculture and describe their changes in relation to ocean and climate variability and \n",
      "change.\n",
      "\n",
      "\n",
      "\n",
      "Cluster 5:\n",
      "\n",
      "Cluster 6:\n",
      "\n",
      "Cluster 7:\n"
     ]
    }
   ],
   "source": [
    "kw_model = KeyBERT()\n",
    "stop_words = stopwords.words('english')\n",
    "#df['keywords'] = df['Summary'].apply(lambda x: kw_model.extract_keywords(x, keyphrase_ngram_range=(1, 1), stop_words=stop_words))\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "num_examples = 10 # Adjust this value based on your needs\n",
    "\n",
    "for cluster_idx in range(8):\n",
    "    cluster_data = np.array(scaled_embeddings)[clusters == cluster_idx]\n",
    "    distances = [np.linalg.norm(embedding - kmeans.cluster_centers_[cluster_idx]) for embedding in cluster_data]\n",
    "    closest_examples_idx = np.argsort(distances)[:num_examples]\n",
    "\n",
    "    print(f\"\\nCluster {cluster_idx}:\")\n",
    "    for idx in closest_examples_idx:\n",
    "        print(f\"Department: {df.loc[idx, 'Agency']}\")\n",
    "        print(f\"Title: {df.loc[idx, 'Title']}\")\n",
    "        print(f\"Summary: {df.loc[idx, 'Summary']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd598c-5aea-4646-a53c-33acd298931d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_keywords = set()\n",
    "for keywords in df['keywords']:\n",
    "    all_keywords.update(kw for kw, _ in keywords if isinstance(kw, tuple))\n",
    "\n",
    "cluster_keyphrases_unigrams = {}\n",
    "for cluster in df['cluster'].unique():\n",
    "    cluster_data = df[df['cluster'] == cluster]\n",
    "    print(cluster_data)\n",
    "    keyphrase_scores_unigrams = {}\n",
    "    for keyword in all_keywords:\n",
    "        scores_unigrams = []\n",
    "        for kws in cluster_data['keywords']:\n",
    "            for item in kws:\n",
    "                kw, score = item\n",
    "                scores_unigrams.append(score)\n",
    "        scores_unigrams = [score for kws in cluster_data['keywords'] if kws and kws[0][0] == keyword for _, score in kws]\n",
    "        if scores_unigrams:\n",
    "            median_value = np.median(scores_unigrams)\n",
    "            keyphrase_scores_unigrams[keyword] = (median_value * len(scores_unigrams)).round(3)\n",
    "    cluster_keyphrases_unigrams[cluster] = sorted(keyphrase_scores_unigrams.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "print(cluster_keyphrases_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f961806d-7f4e-4a6c-9fb0-240c7ee19fab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Keywords1'] = df['Summary'].apply(lambda x: kw_model.extract_keywords(x, keyphrase_ngram_range=(1, 1), stop_words=stop_words))\n",
    "df['Keywords2'] = df['Summary'].apply(lambda x: kw_model.extract_keywords(x, keyphrase_ngram_range=(2, 2), stop_words=stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15c39ff1-506e-44d2-9dda-565f609c77ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/skacholia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/skacholia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/skacholia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ai_detection_image                   ai_document_nlp  \\\n",
      "0          (deep learning, 4.377)         (natural language, 5.472)   \n",
      "1      (satellite imagery, 2.671)       (language processing, 4.95)   \n",
      "2            (sensor data, 1.484)           (processing nlp, 4.924)   \n",
      "3     (cough vocalization, 1.249)         (machine learning, 2.679)   \n",
      "4      (ai classification, 1.229)  (artificial intelligence, 2.355)   \n",
      "5       (vocalization fcv, 1.134)           (text analytics, 1.918)   \n",
      "6          (lidar imagery, 1.122)            (virtual agent, 1.821)   \n",
      "7           (algal blooms, 1.084)        (grant application, 1.768)   \n",
      "8   (analysis reclamation, 1.061)                (nlp model, 1.634)   \n",
      "9             (land cover, 1.052)              (damal tools, 1.588)   \n",
      "10            (video data, 1.046)                   (uses ai, 1.53)   \n",
      "11               (ai used, 1.032)                (ehss data, 1.378)   \n",
      "12   (handheld ultrasound, 1.012)           (topic modeling, 1.314)   \n",
      "13       (computer vision, 1.012)        (understanding nlu, 1.307)   \n",
      "14             (ai detect, 1.004)                (nlu model, 1.231)   \n",
      "\n",
      "                 forecast_model_data                        model_ai_data  \\\n",
      "0          (machine learning, 1.757)            (machine learning, 6.502)   \n",
      "1      (predictive scenarios, 1.548)                       (ai ml, 2.705)   \n",
      "2        (water availability, 1.516)               (deep learning, 2.679)   \n",
      "3              (water quality, 1.42)                  (cfd models, 1.614)   \n",
      "4        (inventory shortage, 1.365)                (neural network, 1.3)   \n",
      "5          (available models, 1.279)                    (big data, 1.272)   \n",
      "6   (predictions groundwater, 1.251)      (reinforcement learning, 1.264)   \n",
      "7            (gpa production, 1.231)              (energy storage, 1.196)   \n",
      "8       (generate predictive, 1.209)                 (predict co2, 1.145)   \n",
      "9              (data sources, 1.173)           (boiler operations, 1.144)   \n",
      "10              (models used, 1.152)          (boiler operational, 1.096)   \n",
      "11         (rainfall outlook, 1.133)                (fired boiler, 1.074)   \n",
      "12       (seasonal forecasts, 1.034)            (fracture warning, 1.073)   \n",
      "13         (mission planning, 1.027)            (physics informed, 1.073)   \n",
      "14               (nih campus, 1.006)  (combustion characteristics, 1.063)   \n",
      "\n",
      "                      ai_data_model  \n",
      "0         (machine learning, 5.204)  \n",
      "1              (claims data, 1.813)  \n",
      "2            (drug labeling, 1.788)  \n",
      "3           (health records, 1.692)  \n",
      "4            (random forest, 1.631)  \n",
      "5            (clinical data, 1.558)  \n",
      "6                 (ehr data, 1.465)  \n",
      "7               (opioid use, 1.398)  \n",
      "8               (drug costs, 1.371)  \n",
      "9                 (ai model, 1.363)  \n",
      "10        (natural language, 1.342)  \n",
      "11        (drug repurposing, 1.276)  \n",
      "12  (artificial intelligence, 1.25)  \n",
      "13            (colon polyps, 1.221)  \n",
      "14      (veterans hepatitis, 1.181)  \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download the required NLTK data if not already present\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize the stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Extract all unique keywords and perform stemming or lemmatization\n",
    "all_keywords = set()\n",
    "for keywords in df['Keywords1']:\n",
    "    for keyword, _ in keywords:\n",
    "        keyword = lemmatizer.lemmatize(keyword)\n",
    "        all_keywords.add(keyword)\n",
    "\n",
    "# Extract all unique keywords and perform stemming or lemmatization\n",
    "for keywords in df['Keywords2']:\n",
    "    for keyword, _ in keywords:\n",
    "        keyword = lemmatizer.lemmatize(keyword)\n",
    "        all_keywords.add(keyword)\n",
    "\n",
    "cluster_keyphrases_bigrams = {}\n",
    "cluster_keyphrases_unigrams = {}\n",
    "\n",
    "for cluster in df['cluster'].unique():\n",
    "    cluster_data = df[df['cluster'] == cluster]\n",
    "\n",
    "    keyphrase_scores_bigrams = {}\n",
    "    keyphrase_scores_unigrams = {}\n",
    "\n",
    "    for keyword in all_keywords:\n",
    "        \n",
    "        # Scores for bigrams\n",
    "        scores_bigrams = [score for kws in cluster_data['Keywords2'] for kw, score in kws if lemmatizer.lemmatize(kw) == keyword]\n",
    "        if scores_bigrams:\n",
    "            median_value = np.median(scores_bigrams)\n",
    "            count = len(scores_bigrams)\n",
    "            keyphrase_scores_bigrams[keyword] = (median_value * count).round(3)\n",
    "        \n",
    "        # Scores for unigrams\n",
    "        scores_unigrams = [score for kws in cluster_data['Keywords1'] for kw, score in kws if lemmatizer.lemmatize(kw) == keyword]\n",
    "        if scores_unigrams:\n",
    "            median_value = np.median(scores_unigrams)\n",
    "            count = len(scores_unigrams)\n",
    "            keyphrase_scores_unigrams[keyword] = (median_value * count).round(3)\n",
    "\n",
    "    sorted_keyphrases_bigrams = sorted(keyphrase_scores_bigrams.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "    sorted_keyphrases_unigrams = sorted(keyphrase_scores_unigrams.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "\n",
    "    cluster_keyphrases_bigrams[cluster] = sorted_keyphrases_bigrams\n",
    "    cluster_keyphrases_unigrams[cluster] = sorted_keyphrases_unigrams\n",
    "\n",
    "cluster_df_bigrams = pd.DataFrame.from_dict(cluster_keyphrases_bigrams)\n",
    "cluster_df_unigrams = pd.DataFrame.from_dict(cluster_keyphrases_unigrams)\n",
    "\n",
    "# Change names of columns based on top unigrams\n",
    "new_columns = ['_'.join([keyword for keyword, _ in cluster_df_unigrams[col].head(3)]) for col in cluster_df_bigrams.columns]\n",
    "cluster_df_bigrams.columns = new_columns\n",
    "cluster_df_unigrams.columns = new_columns\n",
    "\n",
    "print(cluster_df_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00f47b35-a848-4491-8f6f-331e2be6fcf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ffdc4c5-12bb-4648-bd4e-d331a9a52783",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfcopy = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d1e4ca6-9c2c-4f01-8e44-823bbe2f7d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfcopy.to_csv(\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "381b5450-e974-4595-83d0-ee333be30d39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfcopy.to_csv(\"federalai_embed.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
